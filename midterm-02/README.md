# ğŸ¦ Bank Marketing Campaign Classifier

![Python](<https://img.shields.io/badge/python-3.12+-blue.svg>)
![License](<https://img.shields.io/badge/license-MIT-green.svg>)
![Status](<https://img.shields.io/badge/status-live-brightgreen.svg>)
![ML](<https://img.shields.io/badge/ML-XGBoost-orange.svg>)
![AWS](<https://img.shields.io/badge/AWS-EC2-ff9900.svg>)
![Docker](<https://img.shields.io/badge/docker-containerized-blue.svg>)

This project predicts whether a bank customer will subscribe to a term deposit, based on direct marketing campaign data. Built with a complete ML workflow from EDA to deployment, it includes interactive visualization, multiple model comparisons, API services, and a Streamlit UIâ€”deployed with Docker on AWS EC2.

*An end-to-end ML pipeline for customer targeting using the UCI Bank Marketing Dataset*

---

## ğŸš€ Quick Start

### Try the Live Demo
ğŸ“ **Live App**: [<https://deposit-predictor.streamlit.app/>](<https://deposit-predictor.streamlit.app/>)

### Run Locally
```bash
# Clone and setup
git clone <https://github.com/yourusername/bank-campaign-classifier.git>
cd bank-campaign-classifier
pipenv install && pipenv shell

# Train model
python train.py

# Start API server
python predict.py

# Run Streamlit app (in new terminal)
streamlit run streamlit_app.py

```

---

## ğŸ“‹ Table of Contents

- [ğŸ“Œ Problem Statement](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#problem-statement)
- [ğŸ¯ Project Goals](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#project-goals)
- [ğŸ“Š Dataset Information](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#dataset-information)
- [ğŸ“ Project Structure](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#project-structure)
- [âš™ï¸ Setup and Installation](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#setup-and-installation)
- [ğŸ” Exploratory Data Analysis](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#exploratory-data-analysis)
- [ğŸ§  Model Training & Selection](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#model-training--selection)
- [ğŸ“ˆ Results & Model Performance](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#results--model-performance)
- [ğŸš€ Deployment](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#deployment)
- [ğŸ’¡ Usage Examples](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#usage-examples)
- [ğŸ”Œ API Reference](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#api-reference)
- [ğŸ¤ Contributing](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#contributing)
- [ğŸ‘¤ Author](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#author)
- [ğŸ“„ License](notion://www.notion.so/22ec658b02618042b215cae1319dbdae?showMoveTo=true&saveParent=true#license)

---

## ğŸ“Œ Problem Statement

Marketing calls are expensive and time-consuming. However, only a small proportion of customers actually subscribe to financial products like term deposits. This project aims to help banks predict, *before making contact*, which customers are most likely to say yesâ€”so that campaigns can be smarter, leaner, and more effective.

---

## ğŸ¯ Project Goals

- Clean and analyze the UCI Bank Marketing dataset
- Visualize categorical and numerical feature impacts
- Address class imbalance and data leakage issues
- Train and evaluate multiple models (Logistic Regression, Random Forest, XGBoost)
- Select the best model using cross-validation and metric analysis
- Deploy the final model to an API service
- Create an interactive web front end using Streamlit
- Containerize and deploy via Docker to AWS EC2

---

## ğŸ“Š Dataset Information

### ğŸ”¹ Source

- UCI Machine Learning Repository: [Bank Marketing Dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing)

### ğŸ”¹ Target Distribution

[Target Distribution](https://via.placeholder.com/600x400/2E86AB/FFFFFF?text=Target+Distribution+Chart%0A%0A88.7%25+No+%7C+11.3%25+Yes%0A%0AClass+Imbalance+Visualization)

*Severe class imbalance: Only 11.3% of customers subscribed to term deposits - this drives our model selection strategy*

### ğŸ”¹ Stats

- 41,188 rows
- 20 input features
- Binary target: `y` (yes/no for term deposit subscription)
- **Class ratio**: 88.7% No, 11.3% Yes

### ğŸ”¹ Feature Groups

- **Demographic**: Age, job, marital, education
- **Financial**: Default, housing loan, personal loan
- **Communication**: Contact type, month, day of week
- **Campaign**: Number of contacts, outcomes of previous attempts

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data/                     # Dataset storage
â”‚   â””â”€â”€ bank-additional-full.csv     # Bank Marketing Dataset
â”œâ”€â”€ images/                   # Visualization assets
â”‚   â”œâ”€â”€ target_distribution.png      # Replace placeholder with actual chart
â”‚   â”œâ”€â”€ correlation_heatmap.png      # Replace placeholder with actual heatmap
â”‚   â”œâ”€â”€ model_comparison.png         # Replace placeholder with actual comparison
â”‚   â”œâ”€â”€ xgboost_roc_curve.png       # Replace placeholder with actual ROC curve
â”‚   â”œâ”€â”€ duration_log_plot.png       # Replace placeholder with actual plot
â”‚   â””â”€â”€ pdays_log_plot.png          # Replace placeholder with actual plot
â”œâ”€â”€ Notebook.ipynb          # Full EDA and model comparison
â”œâ”€â”€ train.py                 # Training script
â”œâ”€â”€ predict.py               # Inference service
â”œâ”€â”€ streamlit_app.py         # Streamlit web UI
â”œâ”€â”€ requirements.txt         # Dependency management
â”œâ”€â”€ .dockerignore           # Docker reference file
â”œâ”€â”€ Dockerfile              # Docker image configuration
â”œâ”€â”€ Pipfile                 # Dependency management
â”œâ”€â”€ Pipfile.lock            # Dependency management
â””â”€â”€ XGBoost_model.bin       # Trained model artifact

```

---

## âš™ï¸ Setup and Installation

### Prerequisites

- Python 3.12+
- Docker
- Pipenv

### ğŸ“¸ Image Setup

**Note**: This README currently uses placeholder images. To display your actual visualizations:

1. **Create images folder**: `mkdir images`
2. **Export plots from your notebook** as PNG files (300 DPI recommended)
3. **Save with exact filenames**:
- `target_distribution.png` - Bar chart showing class imbalance
- `correlation_heatmap.png` - Feature correlation matrix
- `model_comparison.png` - Model performance comparison chart
- `xgboost_roc_curve.png` - ROC curve for final model
- `duration_log_plot.png` - Log-transformed duration analysis
- `pdays_log_plot.png` - Log-transformed pdays analysis
1. **Commit and push** to GitHub - images will automatically replace placeholders

### Environment Setup

```bash
# Clone repository
git clone <https://github.com/yourusername/bank-campaign-classifier.git>
cd bank-campaign-classifier

# Setup with Pipenv (recommended)
pip install pipenv
pipenv install --dev
pipenv shell

# Alternative with pip
pip install -r requirements.txt

# Verify installation
python -c "import xgboost, streamlit, pandas; print('All dependencies installed!')"

```

### Docker Setup

```bash
# Build image
docker build -t bank-classifier .

# Run container
docker run -p 9696:9696 bank-classifier

```

---

## ğŸ” Exploratory Data Analysis

Conducted in detail in `Notebook.ipynb`:

### Feature Correlation Analysis

[Correlation Heatmap](https://via.placeholder.com/600x500/E74C3C/FFFFFF?text=Correlation+Heatmap%0A%0AFeature+vs+Feature+Correlations%0A%0ALow+individual+correlations%0Awith+target+variable)

*Low individual correlations justify ensemble methods that excel at finding feature interactions*

### Data Quality & Feature Engineering

### Features Removed Due to Data Issues

### Duration Analysis - Data Leakage Detected

[Duration Distribution](https://via.placeholder.com/600x400/F39C12/FFFFFF?text=Duration+Log+Distribution%0A%0ALog%28duration%2B1%29+by+Target%0A%0AClear+separation+by+outcome%0A%0AData+leakage+identified)

*Log(+1) transformed duration shows clear separation by target - removed due to data leakage*

**Why removed**: `duration` is only known AFTER the call ends, making it unavailable for real-time prediction. While highly predictive (customers who talk longer often subscribe), it creates data leakage.

### Pdays Analysis - Uninformative Feature

[Pdays Distribution](https://via.placeholder.com/600x400/9B59B6/FFFFFF?text=Pdays+Log+Distribution%0A%0ALog%28pdays%2B1%29+Analysis%0A%0A96.4%25+missing+values+%28999%29%0A%0AToo+sparse+to+be+useful)

*Log(+1) transformed pdays dominated by 999 values (no previous contact)*

**Why removed**: `pdays` contained 96.4% missing values (coded as 999), providing minimal predictive value. The feature was too sparse to be useful.

### Key Preprocessing Steps

- Column cleanup: Replaced `.` with `_` for consistency
- Education mapping: 8 categories â†’ 4 (primary, secondary, tertiary, unknown)
- Target encoding: `y` converted to binary (1 for yes, 0 for no)
- **Removed problematic features**: `duration` (leakage), `pdays` (sparse), economic indicators
- **Class imbalance**: Addressed with `scale_pos_weight` in final model

### Final Dataset Summary

- **Rows**: 41,188 customers
- **Features**: 16 (after removing problematic variables)
- **Target balance**: 88.7% No, 11.3% Yes
- **Missing values**: Minimal after cleaning

---

## ğŸ§  Model Training & Selection

### Models Trained

1. **Logistic Regression** - Baseline linear model
2. **Random Forest** - Ensemble method with feature importance
3. **XGBoost** - Gradient boosting with advanced regularization

### Techniques Used

- Train/Val/Test split with stratification
- Cross-validation with 5 folds
- Hyperparameter tuning with GridSearchCV
- Threshold selection using F1 optimization
- Class imbalance handling with `scale_pos_weight`

### Model Performance Comparison

[Model Comparison](https://via.placeholder.com/700x500/27AE60/FFFFFF?text=Model+Performance+Comparison%0A%0ALogistic+Regression+%7C+Random+Forest+%7C+XGBoost%0A%0AAccuracy%2C+ROC+AUC%2C+F1%2C+Precision%2C+Recall%0A%0AXGBoost+wins+on+key+metrics)

### Detailed Results

| Metric | Logistic Regression | Random Forest | XGBoost |
| --- | --- | --- | --- |
| Accuracy | 0.872 | **0.878** | 0.873 |
| ROC AUC | 0.765 | 0.773 | **0.778** |
| F1 Score | 0.441 | 0.473 | **0.474** |
| Precision | 0.434 | **0.459** | 0.445 |
| Recall | 0.447 | 0.488 | **0.508** |

### âœ… **Why XGBoost?**

As the XGBoost and Random Forest models had similar performance, I decided that XGBoost is the better model to proceed with. This is due to the fact that the features in the dataset have relatively low individual correlations with the target variable `y`. XGBoost models excel in finding feature interactions and non-linear patterns in datasets. Random Forest models tend to struggle with weak individual signals. As this dataset also has imbalanced classes, XGBoostâ€™s sophistication is extremely useful.

**Key advantages:**

- Best ROC AUC (0.778) and F1 Score (0.474)
- Superior at capturing feature interactions in low-correlation datasets
- Built-in class imbalance handling with `scale_pos_weight`
- Advanced regularization prevents overfitting

---

## ğŸ“ˆ Results & Model Performance

### XGBoost ROC Performance

[XGBoost ROC Curve](https://via.placeholder.com/600x500/3498DB/FFFFFF?text=XGBoost+ROC+Curve%0A%0AAUC%3A+0.778%0A%0ATrue+Positive+Rate%0Avs+False+Positive+Rate%0A%0AStrong+discriminative+ability)

*ROC AUC: 0.778 - Strong discriminative ability for identifying potential subscribers*

### Final Model Metrics

- **Test Accuracy**: 87.3%
- **ROC AUC**: 0.778
- **F1 Score**: 0.474
- **Precision**: 44.5% (low false positives)
- **Recall**: 50.8% (captures half of actual subscribers)

*Model optimized for F1 score to balance precision-recall trade-off in imbalanced dataset*

### Top Predictive Features

| Rank | Feature | Importance | Business Insight |



### Key Business Insights

ğŸ¯ **Actionable Marketing Strategies:**

- **Target previous responders**: Customers who responded positively to past campaigns are 4x more likely to subscribe - prioritize these customers
- **Use cellular contact**: Cellular contact achieves 60% higher success rates than telephone - update contact preferences
- **Time campaigns strategically**: March, June, and October are optimal months - avoid summer/winter campaigns
- **Focus on retired customers**: Highest conversion rates among job categories - create targeted retirement products
- **Screen for credit defaults**: Customers without defaults show significantly higher subscription rates
- **Limit contact frequency**: Excessive contacts in current campaign reduce success probability

ğŸ’¡ **ROI Optimization**: Focus 80% of campaign budget on customers scoring above 0.6 probability threshold for maximum efficiency.

---

## ğŸš€ Deployment

### ğŸŒ Live Application Architecture

**Frontend**: Streamlit web application for interactive predictions

**Backend**: FastAPI service providing RESTful API (port 9696)

**Model**: Serialized XGBoost model (.bin format)

**Infrastructure**: AWS EC2 with Docker containerization

**Monitoring**: Health check endpoints for service monitoring

### ğŸ“ Live Deployment

ğŸ”— **Streamlit App**: https://deposit-predictor.streamlit.app/

ğŸ”— **API Endpoint**: `http://your-ec2-instance:9696`

### AWS EC2 Deployment Steps

1. **Launch EC2 Instance**
    
    ```bash
    # t2.micro or larger recommended
    # Ubuntu 20.04 LTS AMI
    # Security group: Allow HTTP (80), HTTPS (443), Custom TCP (9696)
    
    ```
    
2. **Install Dependencies**
    
    ```bash
    # Connect to EC2 instance
    sudo apt update
    sudo apt install docker.io git -y
    sudo systemctl start docker
    sudo systemctl enable docker
    sudo usermod -aG docker ubuntu
    
    ```
    
3. **Deploy Application**
    
    ```bash
    # Clone and build
    git clone <https://github.com/yourusername/bank-campaign-classifier.git>
    cd bank-campaign-classifier
    sudo docker build -t bank-classifier .
    
    # Run with auto-restart
    sudo docker run -d --name bank-app --restart unless-stopped -p 9696:9696 bank-classifier
    
    ```
    
4. **Verify Deployment**
    
    ```bash
    # Check container status
    sudo docker ps
    
    # Test health endpoint
    curl <http://localhost:9696/health>
    
    ```
    

### Local Development

```bash
# Build image
docker build -t bank-classifier .

# Run container
docker run -p 9696:9696 bank-classifier

# Development mode with volume mounting
docker run -p 9696:9696 -v $(pwd):/app bank-classifier

```

### ğŸ“Š Performance Monitoring

- **Health Check**: `GET /health` endpoint for monitoring
- **Response Time**: Average < 200ms for predictions
- **Throughput**: Handles 100+ concurrent requests
- **Uptime**: 99.9% availability on AWS EC2

---

## ğŸ’¡ Usage Examples

### API Usage

```bash
# Start the prediction service
python predict.py

# Make a prediction
curl -X POST <http://localhost:9696/predict> \\
  -H "Content-Type: application/json" \\
  -d '{
    "job": "management",
    "marital": "single",
    "education": "tertiary",
    "age": 35,
    "campaign": 2,
    "previous": 0,
    "poutcome": "unknown",
    "contact": "cellular",
    "month": "mar",
    "default": "no"
  }'

```

### Streamlit Interface

1. **Navigate to the live app** or run locally
2. **Fill in customer details** using the interactive form
3. **Click â€œPredictâ€** to get subscription probability
4. **View results** with probability score and recommendation

### Python Integration

```python
import requests

# Prepare customer data
customer_data = {
    "job": "admin.",
    "marital": "married",
    "education": "secondary",
    "age": 42,
    "campaign": 1
}

# Get prediction
response = requests.post(
    "<http://localhost:9696/predict>",
    json=customer_data
)
result = response.json()
print(f"Subscription probability: {result['probability']:.2%}")

```

---

## ğŸ”Œ API Reference

### Endpoints

- `POST /predict` - Get prediction for a single customer
- `GET /health` - Health check

### Request Format

```json
{
  "job": "management",
  "marital": "single",
  "education": "tertiary",
  "age": 30,
  "campaign": 1,
  "previous": 0,
  "poutcome": "unknown",
  "contact": "cellular",
  "month": "may",
  "day_of_week": "mon",
  "default": "no",
  "housing": "yes",
  "loan": "no"
}

```

### Response Format

```json
{
  "prediction": 1,
  "probability": 0.73,
  "recommendation": "High probability customer - prioritize for campaign"
}

```

### Input Validation

- **Required fields**: All customer attributes must be provided
- **Valid values**: Categorical fields must match training data categories
- **Data types**: Age and campaign must be integers

---


## ğŸ‘¤ Author

**Your Name**

- GitHub: [@yourusername](https://github.com/yourusername)
- LinkedIn: [Your LinkedIn](https://linkedin.com/in/yourprofile)
- Email: [your.email@example.com](mailto:your.email@example.com)

---

## ğŸ“„ License

This project is licensed under the MIT License. See the <LICENSE> file for details.

---

## ğŸ”— Additional Resources

- [UCI Bank Marketing Dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Docker Documentation](https://docs.docker.com/)
